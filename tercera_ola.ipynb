{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_get_by_name(fasta, ids, delimit=\" \", column=0):\n",
    "\tseq = {}\n",
    "\tkey = \"\"\n",
    "\tarray = []\n",
    "\tsearch = {}\n",
    "\t# keep fasta in dict\n",
    "\twith open(fasta,'r') as f:        \n",
    "\t\tfor line in f: \n",
    "\t\t\tline = line.rstrip()\n",
    "\t\t\tif '>' in line:\n",
    "\t\t\t\tkey = line.replace('>', '')\n",
    "\t\t\t\tseq[key] = \"\"\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tseq[key] = seq[key] + line\n",
    "\t# create pandas dataframe from dict\n",
    "\tfor k in seq.keys():\n",
    "\t\tid_ = k.split(delimit)[column]\n",
    "\t\tarray.append([k, id_, seq[k]])\n",
    "\tdf = pd.DataFrame(array)\n",
    "\tdf.columns = ['header', 'id', 'seq']\n",
    "\t# seq selecciotion\n",
    "\tdf = df.loc[df['id'].isin(ids)]\n",
    "\tfor i in df.index:\n",
    "\t\tsearch[df.loc[i]['header']] = df.loc[i]['seq']\n",
    "\treturn search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paths generales\n",
    "base = Path('/mnt/NAS/projects/virus_SSPA/COVID19_SSPA/')\n",
    "nextstrain_data = base / 'nextstrain_all/data'\n",
    "wave1_path = base / \"other_data/1wave_data/\"\n",
    "ssp = base / 'secuenciacion_salud_publica/'\n",
    "qc_path = ssp / 'analysis_MN908947/all_run_QC_results/'\n",
    "metadata_path = ssp / 'metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QC muetras\n",
    "### Genera el dataframe samples original\n",
    "### fichero con los path de cada batch\n",
    "bp_file = ssp / 'sarscov2_sequencing_analysis_info.csv'\n",
    "bp = pd.read_csv(bp_file, low_memory=False)\n",
    "\n",
    "### concateno las tablas de samples all qc\n",
    "qc_array = []\n",
    "for b, i, v in bp[['run_id', 'path_QC', 'viralrecon_version']].values:\n",
    "    qc = pd.read_csv(i, dtype={'laboratoryID':str, 'batch':str})\n",
    "    \n",
    "    # estas columnas solo estan en algunas de los primeros qc\n",
    "    drop_col = ['qc_nextclade', 'totalGaps_nextclade', 'totalMutations_nextclade']\n",
    "    qc.drop(columns=drop_col, inplace=True, errors='ignore')\n",
    "    \n",
    "    if v != 'iontorrent':\n",
    "        qc['tecnología'] = 'illumina'\n",
    "    else: qc['tecnología'] = v\n",
    "        \n",
    "    if b == 'AND_2_husc_010221':\n",
    "        # elimina el 1442 al inicio que no debía de estar ahí ya que viene de un id de roche\n",
    "        qc['laboratoryID'] = qc['laboratoryID'].apply(lambda x: re.sub(r'^1442', '', x))\n",
    "    qc_array.append(qc)\n",
    "\n",
    "samples = pd.concat(qc_array)\n",
    "samples.reset_index(inplace=True, drop=True)\n",
    "\n",
    "### Elimino los prefijos que se utilizan en HUSC jusnto a los ID de laboratorio\n",
    "samples.loc[samples['laboratoryID'].str.len() == 12, 'laboratoryID'] = samples['laboratoryID'].str.replace(r'^1442', '', regex=True)\n",
    "samples.loc[samples['laboratoryID'].str.len() == 12, 'laboratoryID'] = samples['laboratoryID'].str.replace(r'^1012', '', regex=True)\n",
    "samples.loc[samples['laboratoryID'].str.len() == 10, 'laboratoryID'] = samples['laboratoryID'].str.replace(r'^47', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mapping de identificadores\n",
    "### concateno las tablas de mapping ids\n",
    "lab_array = []\n",
    "\n",
    "for b in bp['run_id'].values:\n",
    "    if 'huvr' in b:\n",
    "        lab = pd.read_csv(f'{metadata_path}/huvr/{b}/{b}_sampleIDlabo_mapping_andID.csv', header=None, dtype={2:str})\n",
    "    elif 'husc' in b:\n",
    "        lab = pd.read_csv(f'{metadata_path}/husc/{b}/{b}_sampleIDlabo_mapping_andID.csv', header=None, dtype={2:str})\n",
    "        if b == 'AND_2_husc_010221':\n",
    "            # elimina el 1442 al inicio que no debía de estar ahí ya que viene de un id de roche\n",
    "            lab[2] = lab[2].apply(lambda x: re.sub(r'^1442', '', x)) \n",
    "    elif 'huvn' in b:\n",
    "        lab = pd.read_csv(f'{metadata_path}/huvn/{b}/{b}_sampleIDlabo_mapping_andID.csv', header=None, dtype={2:str})\n",
    "    elif 'hrum' in b:\n",
    "        lab = pd.read_csv(f'{metadata_path}/hrum/{b}/{b}_sampleIDlabo_mapping_andID.csv', header=None, dtype={2:str})\n",
    "    lab_array.append(lab)\n",
    "    \n",
    "mapping = pd.concat(lab_array)\n",
    "mapping.columns = ['sample', 'and_id', 'num_lab']\n",
    "mapping.reset_index(inplace=True, drop=True)\n",
    "# remove complete duplicated\n",
    "mapping = mapping.loc[~mapping.duplicated(keep='first')].copy()\n",
    "mapping.reset_index(inplace=True, drop=True)\n",
    "# remove some prefix\n",
    "mapping.loc[mapping['num_lab'].str.len() == 12, 'num_lab'] = mapping['num_lab'].str.replace(r'^1442', '', regex=True)\n",
    "mapping.loc[mapping['num_lab'].str.len() == 12, 'num_lab'] = mapping['num_lab'].str.replace(r'^1012', '', regex=True)\n",
    "mapping.loc[mapping['num_lab'].str.len() == 10, 'num_lab'] = mapping['num_lab'].str.replace(r'^47', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tablas linajes de pangolin\n",
    "### concateno las tablas de linajes de pangolin\n",
    "lin_array = []\n",
    "for b, i in bp[['run_id', 'analysis_path']].values:\n",
    "    file = f'{i}/pangolin/{b}_pangolin.csv'\n",
    "    lin = pd.read_csv(file, sep=',')\n",
    "    lin_array.append(lin)\n",
    "    \n",
    "### añado los linajes de la primera ola y proyecto nacional\n",
    "wave1_lineage = wave1_path / 'pangolin/andalusia_seq_pangolin.csv'\n",
    "wave1_lineage_df = pd.read_csv(wave1_lineage, sep=',')\n",
    "lin_array.append(wave1_lineage_df)\n",
    "\n",
    "lineage = pd.concat(lin_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### concateno las tablas de nextclade\n",
    "nextclade_array = []\n",
    "for a_path, run in bp[['analysis_path', 'run_id']].values:\n",
    "    file = Path(a_path) / f'nextclade/{run}.csv'\n",
    "    nc = pd.read_csv(file, sep=';')\n",
    "    nextclade_array.append(nc)\n",
    "    \n",
    "### añado los clados de la primera ola\n",
    "wave1_nextclade = wave1_path / 'nextclade/andalusia_seq_nextclade.csv'\n",
    "wave1_nextclade_df = pd.read_csv(wave1_nextclade, sep=';')\n",
    "nextclade_array.append(wave1_nextclade_df)\n",
    "    \n",
    "nextclade = pd.concat(nextclade_array)\n",
    "nextclade = nextclade[['seqName', 'clade', 'clade_display', 'clade_who', 'Nextclade_pango']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqName</th>\n",
       "      <th>clade</th>\n",
       "      <th>clade_display</th>\n",
       "      <th>clade_who</th>\n",
       "      <th>Nextclade_pango</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>55017021</td>\n",
       "      <td>20A</td>\n",
       "      <td>20A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      seqName clade clade_display clade_who Nextclade_pango\n",
       "311  55017021   20A           20A       NaN             B.1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextclade.loc[nextclade['seqName'] == \"55017021\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning!! 1148 números de laboratorio duplicados en el dataset metadata\n",
      "      IDlaboratorio         NUHSA\n",
      "36708      10130349  AN0285479283\n",
      "36707      10130349  AN0285479283\n",
      "20510      10139710  AN0067854834\n",
      "43550      10139710  AN0067854834\n",
      "5300      102057073  an0472741625\n"
     ]
    }
   ],
   "source": [
    "### Tablas de metadatos\n",
    "### Concateno los fichero de metadatos\n",
    "dtype = {\n",
    "    'IDlaboratorio':str, \n",
    "    'Código Postal':str, \n",
    "    'Edad (años)':\"Int64\",\n",
    "    'Nº Contactos identificados':str,\n",
    "    'Identificador':\"Int64\",\n",
    "    'Tipo':str\n",
    "}\n",
    "\n",
    "remove_cols = [\n",
    "    'Evolución',\n",
    "    'Fecha Alta (dd/mm/aaaa)',\n",
    "    'Fecha de defunción',\n",
    "    'Fue Hospitalizado',\n",
    "    'Hospital de ingreso',\n",
    "    'Identificador',\n",
    "    'Identificador Brote asociado',\n",
    "    'Tipo',\n",
    "    'Ámbito',\n",
    "    'Clasificación reinfección',\n",
    "    'Otros cuadros respiratorios graves',\n",
    "    'Asintomático (sin antecedentes de síntomas)',\n",
    "    'Categoría profesional',\n",
    "    'Contacto estrecho',\n",
    "    'Diabetes',\n",
    "    'Embarazo',\n",
    "    'Enfermedad cardiovascular',\n",
    "    'Enfermedad pulmonar crónica',\n",
    "    'Estancia en UCI',\n",
    "    'Factores de Riesgo y Enfermedad de Base COVID-19',\n",
    "    'Fallo renal agudo',\n",
    "    'Fecha alta UCI',\n",
    "    'Fecha ingreso UCI',\n",
    "    'Hipertensión arterial (HTA)',\n",
    "    'Inmunosupresión',\n",
    "    'Institucionalizados (revisar ámbito)',\n",
    "    'Neumonía',\n",
    "    'Otros factores',\n",
    "    'Otros factores, especificar',\n",
    "    'Síndrome Distress respiratorio',\n",
    "    'Ventilación mecánica',\n",
    "    'Cáncer',\n",
    "    'Estudio Contactos',\n",
    "    'Fecha primera vacuna',\n",
    "    'Fecha segunda vacuna',\n",
    "    'Nº Contactos confirmados como caso',\n",
    "    'Nº Contactos identificados',\n",
    "    'Nombre primera vacuna',\n",
    "    'Nombre segunda vacuna',\n",
    "    'FERE1',\n",
    "    'RE1',\n",
    "    'FERE2',\n",
    "    'RE2',\n",
    "    'FERE3',\n",
    "    'RE3',\n",
    "    'DATABASE RLT2_NUHSA'\n",
    "]\n",
    "\n",
    "# Original mets\n",
    "met1 = pd.read_csv(f'{metadata_path}/huvr/AND_1_huvr_050121/AND_1_huvr_050121_sp.csv', sep='|', dtype=dtype)\n",
    "met2 = pd.read_csv(f'{metadata_path}/husc/AND_2_husc_010221/AND_2_husc_010221_sp.csv', sep='|', dtype=dtype)\n",
    "met3 = pd.read_csv(f'{metadata_path}/huvr/AND_3_huvr_020221/AND_3_huvr_020221_sp.csv', sep='|', dtype=dtype)\n",
    "met4 = pd.read_csv(f'{metadata_path}/husc/AND_4_husc_200221/AND_4_husc_200221_sp.csv', sep='|', dtype=dtype)\n",
    "met5 = pd.read_csv(f'{metadata_path}/huvr/AND_5_huvr_240221/AND_5_huvr_240221_sp.csv', sep='|', dtype=dtype)\n",
    "met6 = pd.read_csv(f'{metadata_path}/huvr/AND_6_huvr_270221/AND_6_huvr_270221_sp.csv', sep='|', dtype=dtype)\n",
    "met7 = pd.read_csv(f'{metadata_path}/huvr/AND_7_huvr_080321/AND_7_huvr_080321_sp.csv', sep=',', dtype=dtype)\n",
    "ori_met = pd.concat([met1, met2, met3, met4, met5, met6, met7])\n",
    "ori_met['ori'] = 'ori_met'\n",
    "\n",
    "### a partir de aquí empezamos con los latest\n",
    "met_huvr = pd.read_csv(f'{metadata_path}/huvr/huvr_acumulativo_latest_sp.csv', sep='|', low_memory=False, dtype=dtype)\n",
    "met_huvr['ori'] = 'met_huvr'\n",
    "met_husc = pd.read_csv(f'{metadata_path}/husc/husc_acumulativo_latest_sp.csv', sep='|', low_memory=False, dtype=dtype)\n",
    "met_husc['ori'] = 'met_husc'\n",
    "met_huvn = pd.read_csv(f'{metadata_path}/huvn/huvn_acumulativo_latest_sp.csv', sep='|', low_memory=False, dtype=dtype)\n",
    "met_huvn['ori'] = 'met_huvn'\n",
    "met_hrum = pd.read_csv(f'{metadata_path}/hrum/hrum_acumulativo_latest.csv', sep='|', low_memory=False, dtype=dtype)\n",
    "met_hrum['ori'] = 'met_hrum'\n",
    "latest_met = pd.concat([met_huvr, met_husc, met_huvn, met_hrum])\n",
    "\n",
    "ori_met = ori_met.loc[(~ori_met['IDlaboratorio'].isin(latest_met['IDlaboratorio'])) & (~ori_met['NUHSA'].isin(latest_met['NUHSA']))]\n",
    "\n",
    "### Este no lo usamos\n",
    "# met_hurm = pd.read_csv(f'{metadata_path}/hrum/hrum_acumulativo_latest.csv', sep=';', dtype={'IDlaboratorio':str, 'Código Postal':str})\n",
    "\n",
    "mets = [latest_met, ori_met]\n",
    "metadata = pd.concat(mets)\n",
    "metadata_original = metadata.copy()\n",
    "metadata.drop(columns=remove_cols, inplace=True)\n",
    "metadata.drop_duplicates(inplace=True, keep='first')\n",
    "metadata.reset_index(inplace=True, drop=False)\n",
    "\n",
    "### guardo en una tabla los ids de laboratorio duplicados\n",
    "num_lab_duplicated = metadata.loc[metadata.duplicated(['IDlaboratorio'], keep=False)].sort_values(by='IDlaboratorio').shape[0]\n",
    "if num_lab_duplicated > 0:\n",
    "    print(f'Warning!! {num_lab_duplicated} números de laboratorio duplicados en el dataset metadata')\n",
    "    print(metadata.loc[metadata.duplicated(['IDlaboratorio'], keep=False), ['IDlaboratorio', 'NUHSA']].sort_values(by='IDlaboratorio').head())\n",
    "metadata.loc[metadata.duplicated('IDlaboratorio', keep=False)].sort_values(by='IDlaboratorio').to_excel('metadata_laboratory_id_duplicated.xlsx', index=False)\n",
    "\n",
    "### elimino los id de laboratorio duplicados quedándome con el primero\n",
    "metadata.drop_duplicates('IDlaboratorio', inplace=True, keep='first')\n",
    "\n",
    "### cambios en los metadatos ad-hoc para que almería empiece por 0\n",
    "metadata.loc[(metadata['Código Postal'].str.len() == 4) & (metadata['Código Postal'].str.match('^4')), 'Código Postal'] = '0' + metadata['Código Postal']\n",
    "\n",
    "#### le quito los espacios en blanco a los nobres de municipio de la tabla metadata\n",
    "metadata['Municipio'] = metadata['Municipio'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Código Postal</th>\n",
       "      <th>Municipio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41091</td>\n",
       "      <td>Sevilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41091</td>\n",
       "      <td>Sevilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41040</td>\n",
       "      <td>Espartinas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41091</td>\n",
       "      <td>Sevilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44904</th>\n",
       "      <td>11405</td>\n",
       "      <td>Jerez de la Frontera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44905</th>\n",
       "      <td>11405</td>\n",
       "      <td>Jerez de la Frontera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44906</th>\n",
       "      <td>11520</td>\n",
       "      <td>Rota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44907</th>\n",
       "      <td>11406</td>\n",
       "      <td>Jerez de la Frontera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44908</th>\n",
       "      <td>29569</td>\n",
       "      <td>Pizarra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44156 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Código Postal             Municipio\n",
       "0               NaN                   NaN\n",
       "1             41091               Sevilla\n",
       "2             41091               Sevilla\n",
       "3             41040            Espartinas\n",
       "4             41091               Sevilla\n",
       "...             ...                   ...\n",
       "44904         11405  Jerez de la Frontera\n",
       "44905         11405  Jerez de la Frontera\n",
       "44906         11520                  Rota\n",
       "44907         11406  Jerez de la Frontera\n",
       "44908         29569               Pizarra\n",
       "\n",
       "[44156 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[['Código Postal', 'Municipio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 43385\n",
      "samples merged witn lineage: 43385\n",
      "samples merged wit nextclade: 43385\n",
      "samples merged witn metadata: 43385\n",
      "samples without duplicated: 43385\n",
      "Warning!! 653 Números de laboratorio duplicados en el dataset samples\n"
     ]
    }
   ],
   "source": [
    "### Unifico las tablas anteriores\n",
    "### Mergeo el dataframe samples con lineages, nextclade y metadata\n",
    "### mapping no se para que sirve y lo voy a quitar\n",
    "\n",
    "print(f'total samples: {samples.shape[0]}')\n",
    "\n",
    "### merge with lineage\n",
    "samples = samples.merge(lineage, how='left', left_on='sample', right_on='taxon')\n",
    "print(f'samples merged witn lineage: {samples.shape[0]}')\n",
    "\n",
    "### merge with nextclade\n",
    "samples = samples.merge(nextclade, how='left', left_on='sample', right_on='seqName')\n",
    "print(f'samples merged wit nextclade: {samples.shape[0]}')\n",
    "\n",
    "\n",
    "### merge with mapping\n",
    "#samples = samples.merge(mapping, how='left', on='sample')\n",
    "#print(f'samples merged witn mapping: {samples.shape[0]}')\n",
    "\n",
    "#num_lab_error = len(samples.loc[samples['laboratoryID'] != samples['num_lab']][['laboratoryID', 'num_lab']].dropna())\n",
    "#if num_lab_error != 0:\n",
    "#    print(f'Warning!! {num_lab_error} Números de laboratorio diferentes en el dataset samples')\n",
    "#    print(samples.loc[samples['laboratoryID'] != samples['num_lab'], ['laboratoryID', 'num_lab']].head(50))\n",
    "    \n",
    "#samples.loc[samples['laboratoryID'].isnull(), 'laboratoryID'] = samples['num_lab']\n",
    "\n",
    "### merge with metadata\n",
    "samples = samples.merge(metadata, how='left', left_on='laboratoryID', right_on='IDlaboratorio')\n",
    "print(f'samples merged witn metadata: {samples.shape[0]}')\n",
    "\n",
    "### Elimino las líneas identicas en el dataframe samples\n",
    "samples.drop_duplicates(inplace=True, keep='first')\n",
    "print(f'samples without duplicated: {samples.shape[0]}')\n",
    "\n",
    "### Compruebo números de laboratorio duplicados en el dataset samples\n",
    "samples_duplicated_labID = samples.loc[samples['laboratoryID'].duplicated(keep=False)].sort_values('laboratoryID').shape[0]\n",
    "if samples_duplicated_labID > 0 :\n",
    "    samples.loc[samples['laboratoryID'].duplicated(keep=False)].sort_values('laboratoryID').to_excel('samples_laboratory_id_duplicated.xlsx', index=False)\n",
    "    print(f'Warning!! {samples_duplicated_labID} Números de laboratorio duplicados en el dataset samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>fastq_sample</th>\n",
       "      <th>laboratoryID</th>\n",
       "      <th>seqPlatform</th>\n",
       "      <th>bioanalyzer_profile</th>\n",
       "      <th>num_input_reads</th>\n",
       "      <th>num_trimmed_reads</th>\n",
       "      <th>percentage_nonhost_reads</th>\n",
       "      <th>percentage_mapping_viral_genome</th>\n",
       "      <th>median_depth</th>\n",
       "      <th>...</th>\n",
       "      <th>Edad (años)</th>\n",
       "      <th>Fecha del caso</th>\n",
       "      <th>Fecha Inicio Síntomas (dd/mm/aaaa)</th>\n",
       "      <th>País del caso</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>Centro de salud</th>\n",
       "      <th>Código Postal</th>\n",
       "      <th>Municipio</th>\n",
       "      <th>Fecha_diag</th>\n",
       "      <th>ori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AND00001</td>\n",
       "      <td>HUVR-2UK-40085344</td>\n",
       "      <td>HUVR-2UK-40085344</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>1440704</td>\n",
       "      <td>1200502</td>\n",
       "      <td>99.851895</td>\n",
       "      <td>99.79</td>\n",
       "      <td>4672.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AND00002</td>\n",
       "      <td>HUVR-7UK-50072114</td>\n",
       "      <td>HUVR-7UK-50072114</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>1297010</td>\n",
       "      <td>1079260</td>\n",
       "      <td>99.752423</td>\n",
       "      <td>99.71</td>\n",
       "      <td>4462.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AND00003</td>\n",
       "      <td>HUVR-4UK-40085183</td>\n",
       "      <td>HUVR-4UK-40085183</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>1427538</td>\n",
       "      <td>1188870</td>\n",
       "      <td>99.896036</td>\n",
       "      <td>99.89</td>\n",
       "      <td>4613.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AND00004</td>\n",
       "      <td>HUVR-6UK-50072094</td>\n",
       "      <td>HUVR-6UK-50072094</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>1351068</td>\n",
       "      <td>1131586</td>\n",
       "      <td>99.945740</td>\n",
       "      <td>99.94</td>\n",
       "      <td>4750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AND00005</td>\n",
       "      <td>HUVR-1UK-201314254</td>\n",
       "      <td>HUVR-1UK-201314254</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>1458662</td>\n",
       "      <td>1229806</td>\n",
       "      <td>99.381041</td>\n",
       "      <td>99.37</td>\n",
       "      <td>4807.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43380</th>\n",
       "      <td>AND43614</td>\n",
       "      <td>41231737</td>\n",
       "      <td>41231737</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>336848</td>\n",
       "      <td>244960</td>\n",
       "      <td>99.997551</td>\n",
       "      <td>99.91</td>\n",
       "      <td>895.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>met_huvn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43381</th>\n",
       "      <td>AND43615</td>\n",
       "      <td>41231741</td>\n",
       "      <td>41231741</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>376174</td>\n",
       "      <td>269114</td>\n",
       "      <td>99.820151</td>\n",
       "      <td>99.7</td>\n",
       "      <td>928.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>met_huvn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43382</th>\n",
       "      <td>AND43616</td>\n",
       "      <td>41231749</td>\n",
       "      <td>41231749</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>308776</td>\n",
       "      <td>224528</td>\n",
       "      <td>99.944773</td>\n",
       "      <td>99.81</td>\n",
       "      <td>823.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>met_huvn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43383</th>\n",
       "      <td>AND43617</td>\n",
       "      <td>41231750</td>\n",
       "      <td>41231750</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>307906</td>\n",
       "      <td>218482</td>\n",
       "      <td>99.972538</td>\n",
       "      <td>99.85</td>\n",
       "      <td>772.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Rosa</td>\n",
       "      <td>14021</td>\n",
       "      <td>Córdoba</td>\n",
       "      <td>27/03/2022</td>\n",
       "      <td>met_huvn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43384</th>\n",
       "      <td>AND43618</td>\n",
       "      <td>41231753</td>\n",
       "      <td>41231753</td>\n",
       "      <td>Illumina</td>\n",
       "      <td>-</td>\n",
       "      <td>301778</td>\n",
       "      <td>218570</td>\n",
       "      <td>99.939607</td>\n",
       "      <td>99.85</td>\n",
       "      <td>738.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>met_huvn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43385 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sample        fastq_sample        laboratoryID seqPlatform  \\\n",
       "0      AND00001   HUVR-2UK-40085344   HUVR-2UK-40085344    Illumina   \n",
       "1      AND00002   HUVR-7UK-50072114   HUVR-7UK-50072114    Illumina   \n",
       "2      AND00003   HUVR-4UK-40085183   HUVR-4UK-40085183    Illumina   \n",
       "3      AND00004   HUVR-6UK-50072094   HUVR-6UK-50072094    Illumina   \n",
       "4      AND00005  HUVR-1UK-201314254  HUVR-1UK-201314254    Illumina   \n",
       "...         ...                 ...                 ...         ...   \n",
       "43380  AND43614            41231737            41231737    Illumina   \n",
       "43381  AND43615            41231741            41231741    Illumina   \n",
       "43382  AND43616            41231749            41231749    Illumina   \n",
       "43383  AND43617            41231750            41231750    Illumina   \n",
       "43384  AND43618            41231753            41231753    Illumina   \n",
       "\n",
       "      bioanalyzer_profile num_input_reads num_trimmed_reads  \\\n",
       "0                       -         1440704           1200502   \n",
       "1                       -         1297010           1079260   \n",
       "2                       -         1427538           1188870   \n",
       "3                       -         1351068           1131586   \n",
       "4                       -         1458662           1229806   \n",
       "...                   ...             ...               ...   \n",
       "43380                   -          336848            244960   \n",
       "43381                   -          376174            269114   \n",
       "43382                   -          308776            224528   \n",
       "43383                   -          307906            218482   \n",
       "43384                   -          301778            218570   \n",
       "\n",
       "       percentage_nonhost_reads percentage_mapping_viral_genome  median_depth  \\\n",
       "0                     99.851895                           99.79        4672.0   \n",
       "1                     99.752423                           99.71        4462.0   \n",
       "2                     99.896036                           99.89        4613.0   \n",
       "3                     99.945740                           99.94        4750.0   \n",
       "4                     99.381041                           99.37        4807.0   \n",
       "...                         ...                             ...           ...   \n",
       "43380                 99.997551                           99.91         895.0   \n",
       "43381                 99.820151                            99.7         928.0   \n",
       "43382                 99.944773                           99.81         823.0   \n",
       "43383                 99.972538                           99.85         772.0   \n",
       "43384                 99.939607                           99.85         738.0   \n",
       "\n",
       "       ...  Edad (años)  Fecha del caso Fecha Inicio Síntomas (dd/mm/aaaa)  \\\n",
       "0      ...         <NA>             NaN                                NaN   \n",
       "1      ...         <NA>             NaN                                NaN   \n",
       "2      ...         <NA>             NaN                                NaN   \n",
       "3      ...         <NA>             NaN                                NaN   \n",
       "4      ...         <NA>             NaN                                NaN   \n",
       "...    ...          ...             ...                                ...   \n",
       "43380  ...         <NA>             NaN                                NaN   \n",
       "43381  ...         <NA>             NaN                                NaN   \n",
       "43382  ...         <NA>             NaN                                NaN   \n",
       "43383  ...         <NA>             NaN                                NaN   \n",
       "43384  ...         <NA>             NaN                                NaN   \n",
       "\n",
       "       País del caso  Sexo  Centro de salud Código Postal Municipio  \\\n",
       "0                NaN   NaN              NaN           NaN       NaN   \n",
       "1                NaN   NaN              NaN           NaN       NaN   \n",
       "2                NaN   NaN              NaN           NaN       NaN   \n",
       "3                NaN   NaN              NaN           NaN       NaN   \n",
       "4                NaN   NaN              NaN           NaN       NaN   \n",
       "...              ...   ...              ...           ...       ...   \n",
       "43380            NaN   NaN              NaN           NaN       NaN   \n",
       "43381            NaN   NaN              NaN           NaN       NaN   \n",
       "43382            NaN   NaN              NaN           NaN       NaN   \n",
       "43383            NaN   NaN       Santa Rosa         14021   Córdoba   \n",
       "43384            NaN   NaN              NaN           NaN       NaN   \n",
       "\n",
       "       Fecha_diag       ori  \n",
       "0             NaN       NaN  \n",
       "1             NaN       NaN  \n",
       "2             NaN       NaN  \n",
       "3             NaN       NaN  \n",
       "4             NaN       NaN  \n",
       "...           ...       ...  \n",
       "43380         NaN  met_huvn  \n",
       "43381         NaN  met_huvn  \n",
       "43382         NaN  met_huvn  \n",
       "43383  27/03/2022  met_huvn  \n",
       "43384         NaN  met_huvn  \n",
       "\n",
       "[43385 rows x 66 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Borrado de muestras incorrectas\n",
    "### Elimino muestras por errores en los datos que nos pasan\n",
    "samples = samples.loc[samples['sample'] != 'AND37139']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43384, 66)\n",
      "(43362, 3)\n",
      "(44156, 17)\n",
      "(43885, 16)\n"
     ]
    }
   ],
   "source": [
    "print(samples.shape)\n",
    "print(mapping.shape)\n",
    "print(metadata.shape)\n",
    "print(lineage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43384, 66)\n",
      "(43362, 3)\n",
      "(44156, 17)\n",
      "(43885, 16)\n"
     ]
    }
   ],
   "source": [
    "print(samples.shape)\n",
    "print(mapping.shape)\n",
    "print(metadata.shape)\n",
    "print(lineage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_ids = set(samples['sample'].values)\n",
    "lin_ids = set(lineage['taxon'].values)\n",
    "\n",
    "len(sam_ids - lin_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50290/1163936512.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples['origen'] = 'SAS'\n",
      "/tmp/ipykernel_50290/1163936512.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples.loc[samples['Código Postal'].str.startswith('41'), 'provincia'] = 'Sevilla'\n",
      "/tmp/ipykernel_50290/1163936512.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples['Municipio'] = samples['Municipio'].str.strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Municipios sin localización\n",
      "11735    Fernán Núñez\n",
      "11736    Fernán Núñez\n",
      "11737    Fernán Núñez\n",
      "11884     Cúllar-Vega\n",
      "11894    Huétor-Tájar\n",
      "11897    Huétor-Tájar\n",
      "11927    Huétor-Tájar\n",
      "11928    Huétor-Tájar\n",
      "11939        Santa Fé\n",
      "12153    Fernán Núñez\n",
      "12155    Fernán Núñez\n",
      "12156    Fernán Núñez\n",
      "12157    Fernán Núñez\n",
      "12158    Fernán Núñez\n",
      "12159    Fernán Núñez\n",
      "12160    Fernán Núñez\n",
      "12161    Fernán Núñez\n",
      "12285    Huétor-Tájar\n",
      "12568    Fernán Núñez\n",
      "12575    Fernán Núñez\n",
      "12576    Fernán Núñez\n",
      "12577    Fernán Núñez\n",
      "12578    Fernán Núñez\n",
      "12579    Fernán Núñez\n",
      "12880    Fernán Núñez\n",
      "12881    Fernán Núñez\n",
      "19483       Taha (La)\n",
      "26760     Cúllar-Vega\n",
      "26769     Cúllar-Vega\n",
      "26813     Cúllar-Vega\n",
      "26850        Santa Fé\n",
      "26851        Santa Fé\n",
      "26853        Santa Fé\n",
      "26854        Santa Fé\n",
      "26855        Santa Fé\n",
      "26856        Santa Fé\n",
      "27531    Fernán Núñez\n",
      "27707     Cúllar-Vega\n",
      "27713        Santa Fé\n",
      "27715        Santa Fé\n",
      "27993     Cúllar-Vega\n",
      "28110    Fernán Núñez\n",
      "28124    Fernán Núñez\n",
      "28275    Fernán Núñez\n",
      "28295    Fernán Núñez\n",
      "28473    Fernán Núñez\n",
      "28685    Fernán Núñez\n",
      "29103        Santa Fé\n",
      "29170        Santa Fé\n",
      "29284        Santa Fé\n",
      "29427    Huétor-Tájar\n",
      "29443        Santa Fé\n",
      "30564    Fernán Núñez\n",
      "37438       Taha (La)\n",
      "42537           V�lor\n",
      "42543        Santa Fé\n",
      "Name: Municipio, dtype: object\n",
      "########\n",
      "Municipios sin provincia\n",
      " Municipio\n",
      "Fernán Núñez    1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50290/1163936512.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples.loc[samples['batch'].str.contains('huvr'), 'hospital de referencia'] = 'HUVR'\n",
      "/tmp/ipykernel_50290/1163936512.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples.loc[samples['provincia'] == 'Sevilla', 'hospital'] = 'Hospital Universitario Virgen del Rocío'\n",
      "/tmp/ipykernel_50290/1163936512.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples.drop_duplicates(keep='first', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 70)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "# Modificaciones posteriores a la tabla samples mergeada con todo\n",
    "\n",
    "#########################################\n",
    "# asigno la fuente\n",
    "samples['origen'] = 'SAS'\n",
    "\n",
    "#########################################\n",
    "# Asigno las provincias por código postal\n",
    "samples.loc[samples['Código Postal'].isnull(), 'Código Postal'] = '0'\n",
    "samples.loc[samples['Código Postal'].str.startswith('41'), 'provincia'] = 'Sevilla'\n",
    "samples.loc[samples['Código Postal'].str.startswith('14'), 'provincia'] = 'Córdoba'\n",
    "samples.loc[samples['Código Postal'].str.startswith('21'), 'provincia'] = 'Huelva'\n",
    "samples.loc[samples['Código Postal'].str.startswith('23'), 'provincia'] = 'Jaén'\n",
    "samples.loc[samples['Código Postal'].str.startswith('18'), 'provincia'] = 'Granada'\n",
    "samples.loc[samples['Código Postal'].str.startswith('29'), 'provincia'] = 'Málaga'\n",
    "samples.loc[samples['Código Postal'].str.startswith('04'), 'provincia'] = 'Almería'\n",
    "samples.loc[samples['Código Postal'].str.startswith('11'), 'provincia'] = 'Cádiz'\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "# conversion de nombres de localidades al formato de la tabla de latitud y longitud\n",
    "samples['Municipio'] = samples['Municipio'].str.strip()\n",
    "\n",
    "location_names = './data/location_names_dict.tsv'\n",
    "with open(location_names,'r') as f:\n",
    "    for line in f: \n",
    "        line = line.strip()\n",
    "        loc1, loc2 = line.split('\\t')\n",
    "        samples.loc[samples['Municipio'] == loc1, 'Municipio'] = loc2\n",
    "        \n",
    "lat_long = pd.read_csv('./config/lat_long.tsv', sep='\\t', names=['location', 'name', 'lat', 'long'])\n",
    "print('Municipios sin localización')\n",
    "print(samples.loc[(~samples['Municipio'].isin(lat_long['name'])) & (samples['Municipio'].notnull()), 'Municipio'])\n",
    "print('########')\n",
    "\n",
    "#######################################################\n",
    "# Asginación de provincias a partir de la localidad (para cuando no hay codigo postal)\n",
    "loc_province = f'./data/location_province_dict.tsv'\n",
    "with open(loc_province,'r') as f:\n",
    "    for line in f: \n",
    "        line = line.rstrip()\n",
    "        loc, province = line.split('\\t')\n",
    "        samples.loc[samples['Municipio'] == loc, 'provincia'] = province\n",
    "        \n",
    "print('Municipios sin provincia\\n', samples.loc[(samples['provincia'].isnull()) & (samples['Municipio'].notnull()), 'Municipio'].value_counts())\n",
    "\n",
    "########################################################################\n",
    "# genera los hospitales y los hospitales de referencia para las muestras\n",
    "\n",
    "# Hospitales de referencia\n",
    "samples.loc[samples['batch'].str.contains('huvr'), 'hospital de referencia'] = 'HUVR'\n",
    "samples.loc[samples['batch'].str.contains('husc'), 'hospital de referencia'] = 'HUSC'\n",
    "\n",
    "# Hospitales recolector\n",
    "samples.loc[samples['provincia'] == 'Sevilla', 'hospital'] = 'Hospital Universitario Virgen del Rocío'\n",
    "samples.loc[samples['provincia'] == 'Córdoba', 'hospital'] = 'Hospital Universitario Reina Sofía'\n",
    "samples.loc[samples['provincia'] == 'Huelva', 'hospital'] = 'Hospital Universitario Juan Ramón Jiménez'\n",
    "samples.loc[samples['provincia'] == 'Jaén', 'hospital'] = 'Hospital Universitario de Jaén'\n",
    "samples.loc[samples['provincia'] == 'Granada', 'hospital'] = 'Hospital Universitario San Cecilio'\n",
    "samples.loc[samples['provincia'] == 'Málaga', 'hospital'] = 'Hospital Universitario Virgen de la Victoria'\n",
    "samples.loc[samples['provincia'] == 'Almería', 'hospital'] = 'Hospital Universitario Torrecárdenas'\n",
    "samples.loc[samples['provincia'] == 'Cádiz', 'hospital'] = 'Hospital Universitario Puerta del Mar'\n",
    "\n",
    "samples.loc[samples['batch'].str.contains('huvn'), 'hospital'] = 'Hospital Universitario Vigen de las Nieves'\n",
    "samples.loc[samples['batch'].str.contains('hurm'), 'hospital'] = 'Hospital Universitario Regional de Málaga'\n",
    "\n",
    "########################################################\n",
    "# Elimina los duplicados.Filas identicas\n",
    "samples.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "########################################################\n",
    "# Completo los metadatos de las secuencias que no tienen asignada plataforma de secuenciación\n",
    "samples.loc[samples['seqPlatform'].isnull(), 'seqPlatform'] = 'Illumina'\n",
    "\n",
    "########################################################\n",
    "# Por peticion de Lepe las muestras del HUVR pasan a tener como centro de secuenciación \"microbiología HU Virgen del Rocio\"\n",
    "samples.loc[samples['batch'].str.contains('huvr'), 'centro_secuenciacion'] = \"microbiología HU Virgen del Rocio\"\n",
    "\n",
    "samples.loc[samples.duplicated(keep=False)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = samples[['Municipio', 'provincia']]\n",
    "\n",
    "temp = temp.loc[temp['Municipio'].notnull()]\n",
    "temp = temp.loc[temp['provincia'].notnull()]\n",
    "temp = temp.drop_duplicates()\n",
    "temp.to_csv('test_location_province.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50290/1130609688.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples['fecha_nextstrain'] = samples['Fecha Inicio Síntomas (dd/mm/aaaa)']\n",
      "/tmp/ipykernel_50290/1130609688.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples['fecha_nextstrain'] = pd.to_datetime(samples['fecha_nextstrain'], format='%d/%m/%Y')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################\n",
      "Andalucia Occidental (Microbiología HUVR)\n",
      "#########################################\n",
      ">>> Fechas formato OK\n",
      ">>> Fechas futuro OK\n",
      ">>> Fechas antiguas: \n",
      "      Identificacion         NUHSA      Fecha\n",
      "21371       14392132  AN0163330823 1967-09-05\n",
      "[['AND26568' '1512915' Timestamp('2022-06-07 00:00:00')]\n",
      " ['AND26568' '1512915' Timestamp('2022-06-07 00:00:00')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50290/1130609688.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  date_occi_errors = pd.concat([date_occi_errors, date_occi[['Identificacion', 'NUHSA', 'Fecha']].loc[date_occi['Fecha'] < '2020-01-01']])\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# Unifico las fechas prodecedentes de los datos de salud pública\n",
    "samples['fecha_nextstrain'] = samples['Fecha Inicio Síntomas (dd/mm/aaaa)']\n",
    "samples.loc[samples['fecha_nextstrain'].isnull(), 'fecha_nextstrain'] = samples['Fecha del caso']\n",
    "samples.loc[samples['fecha_nextstrain'].isnull(), 'fecha_nextstrain'] = samples['Fecha_diag']\n",
    "# Transforma las fecha de string a datetime\n",
    "samples['fecha_nextstrain'] = pd.to_datetime(samples['fecha_nextstrain'], format='%d/%m/%Y')\n",
    "\n",
    "########################################################\n",
    "# Actualizo las fechas según los datos de Microbiología\n",
    "############ Andalucía occidental #####################\n",
    "print('#########################################')\n",
    "print('Andalucia Occidental (Microbiología HUVR)')\n",
    "print('#########################################')\n",
    "date_occident_path = f'{metadata_path}/huvr/MUESTRAS_SECUENCIACION_SEMANALES_acumulada_latest.csv'\n",
    "date_occi = pd.read_csv(date_occident_path, sep=';')\n",
    "\n",
    "### Comprueba que todas las fechas están en formato adecuado\n",
    "date_occi_errors = date_occi[['Identificacion', 'NUHSA', 'Fecha']].loc[pd.to_datetime(date_occi['Fecha'], format='%d/%m/%Y', errors='coerce').isnull()]\n",
    "if date_occi_errors.shape[0] > 0:\n",
    "    print('>>> Fechas formato dd/mm/yyyy: ')\n",
    "    print(date_occi[['Identificacion', 'NUHSA', 'Fecha']].loc[pd.to_datetime(date_occi['Fecha'], format='%d/%m/%Y', errors='coerce').isnull()])\n",
    "    # Me quedo con los errores para mandarlos a micro\n",
    "    date_occi['Fecha'] = pd.to_datetime(date_occi['Fecha'], format='%d/%m/%Y', errors='coerce')\n",
    "else:\n",
    "    print('>>> Fechas formato OK')\n",
    "    date_occi['Fecha'] = pd.to_datetime(date_occi['Fecha'], format='%d/%m/%Y')\n",
    "\n",
    "### Comprueba que no hay fechas del futuro\n",
    "today = datetime.datetime.now()\n",
    "if date_occi.loc[pd.to_datetime(date_occi['Fecha']) > today].shape[0]:\n",
    "    print('>>> Fechas futuro: ')\n",
    "    print(date_occi[['Identificacion', 'NUHSA', 'Fecha']].loc[date_occi['Fecha'] > today])\n",
    "    # Concateno con los errores de formato\n",
    "    if date_occi_errors.shape[0] == 0:\n",
    "        date_occi_errors\n",
    "    else:\n",
    "        date_occi_errors = pd.concat([date_occi_errors, date_occi[['ID', 'NUHSA', 'Fecha']].loc[date_occi['Fecha'] > today]])\n",
    "    date_occi['Fecha'] = date_occi['Fecha'].loc[date_occi['Fecha'] <= today]\n",
    "else:\n",
    "    print('>>> Fechas futuro OK')\n",
    "\n",
    "### Comprueba que no hay fechas anteriores al 1 enero de 2020\n",
    "if date_occi.loc[date_occi['Fecha'] < '2020-01-01'].shape[0]:\n",
    "    print('>>> Fechas antiguas: ')\n",
    "    print(date_occi[['Identificacion', 'NUHSA', 'Fecha']].loc[date_occi['Fecha'] < '2020-01-01'])\n",
    "    # Concateno con el resto de errores de fecha\n",
    "    date_occi_errors = pd.concat([date_occi_errors, date_occi[['Identificacion', 'NUHSA', 'Fecha']].loc[date_occi['Fecha'] < '2020-01-01']])\n",
    "    date_occi['Fecha'] = date_occi['Fecha'].loc[date_occi['Fecha'] >= '2020-01-01']\n",
    "else:\n",
    "    print('>>> Fechas antiguas OK')\n",
    "\n",
    "# Guardo los errores en un fichero para pasarselo a Javier\n",
    "date_occi_errors.to_csv('date_occi_error.tsv', sep='\\t', index=False)\n",
    "\n",
    "# elimino las fechas duplicadas con mismo id\n",
    "date_occi.drop(date_occi.loc[date_occi[['Fecha', 'Identificacion']].duplicated(keep='first')].index, inplace=True)\n",
    "date_occi.drop(date_occi.loc[date_occi[['NUHSA', 'Identificacion']].duplicated(keep='first')].index, inplace=True)\n",
    "\n",
    "# unifico las tablas de samples y microbiología occidental\n",
    "samples = samples.merge(date_occi[['Fecha', 'Identificacion']], how='left', left_on='laboratoryID', right_on='Identificacion')\n",
    "\n",
    "# renombro la columna a fecha mircro y la paso a datetime\n",
    "samples['Fecha_micro'] = samples['Fecha']\n",
    "samples.rename(columns={'Fecha':'Fecha_micro_huvr'}, inplace=True)\n",
    "samples['Fecha_micro'] = pd.to_datetime(samples['Fecha_micro'], format='%m/%d/%Y')\n",
    "\n",
    "############### Modificació para añadir las muestras de la carcel de Córdoba ##################################\n",
    "carcel = date_occi.loc[date_occi['Justificacion clinica'] == 'Brote prisión']\n",
    "carcel = carcel.loc[~carcel['NUHSA'].str.startswith('AN'), ['Identificacion']]\n",
    "carcel_ids = list(carcel['Identificacion'].values)\n",
    "samples.loc[samples['laboratoryID'].isin(carcel_ids), 'Municipio'] = 'Córdoba'\n",
    "samples.loc[samples['laboratoryID'].isin(carcel_ids), 'provincia'] = 'Córdoba'\n",
    "\n",
    "print(samples.loc[samples[['sample', 'laboratoryID']].duplicated(keep=False)][['sample', 'laboratoryID', 'fecha_nextstrain']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################\n",
      "Andalucia Oriental (Microbiología HUSC) #\n",
      "#########################################\n",
      ">>> Fechas formato dd/mm/yy: \n",
      "     ori_temp_id ori_temp_date Fecha de la pueba\n",
      "253     22217049    01/02/1998        01/02/1998\n",
      "437     35219490    24/08/1988        24/08/1988\n",
      "3680    23112077    24/07/1994        24/07/1994\n",
      "5622    30644492    27/10/2010        27/10/2010\n",
      ">>> Fechas futuro OK\n",
      ">>> Fechas antiguas OK\n",
      "Index(['ori_temp_id', 'NUHSA', 'Fecha de la pueba', 'dd', 'mm', 'yy', 'mmint',\n",
      "       'ori_temp_date'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 75)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################\n",
    "# Actualizo las fechas según los datos de Microbiología\n",
    "############ Andalucía oriental ########################\n",
    "print('#########################################')\n",
    "print('Andalucia Oriental (Microbiología HUSC) #')\n",
    "print('#########################################')\n",
    "\n",
    "date_orient_path = f'{ssp}/metadata/husc/Actualizacion_Salud_Publica_latest.csv'\n",
    "#date_orient_path = f'{ssp}/metadata/husc/Actualizacion_Salud_Publica_20240620_all.csv'\n",
    "#date_ori = pd.read_csv(date_orient_path, sep=';')\n",
    "date_ori = pd.read_csv(date_orient_path, sep=';', usecols=['Identificador', 'NUHSA', 'Fecha de la pueba'])\n",
    "\n",
    "# Cambio el nombre de la columna identificador\n",
    "date_ori.rename(columns={'Identificador':'ori_temp_id'}, inplace=True)\n",
    "# Guardo las muestras sin fecha en un dataframe para reportar errores\n",
    "date_ori_errors = date_ori[['ori_temp_id', 'NUHSA', 'Fecha de la pueba']].loc[date_ori['Fecha de la pueba'].isnull()]\n",
    "# Elinimo las muestras sin fecha\n",
    "date_ori = date_ori.loc[date_ori['Fecha de la pueba'].notnull()]\n",
    "# Elimino las fechas sin los divisores correctos\n",
    "date_ori = date_ori.loc[date_ori['Fecha de la pueba'].str.contains('/')]\n",
    "\n",
    "### Elimino los prefijos que se utilizan en HUSC jusnto a los ID de laboratorio\n",
    "date_ori.loc[date_ori['ori_temp_id'].str.len() == 12, 'ori_temp_id'] = date_ori['ori_temp_id'].str.replace(r'^1442', '', regex=True)\n",
    "date_ori.loc[date_ori['ori_temp_id'].str.len() == 12, 'ori_temp_id'] = date_ori['ori_temp_id'].str.replace(r'^1012', '', regex=True)\n",
    "date_ori.loc[date_ori['ori_temp_id'].str.len() == 10, 'ori_temp_id'] = date_ori['ori_temp_id'].str.replace(r'^47', '', regex=True)\n",
    "\n",
    "# cambio los años para ajustarme al formato dd/mm/yy\n",
    "date_ori['Fecha de la pueba'] = date_ori['Fecha de la pueba'].str.replace('/2020', '/20')\n",
    "date_ori['Fecha de la pueba'] = date_ori['Fecha de la pueba'].str.replace('/2021', '/21')\n",
    "date_ori['Fecha de la pueba'] = date_ori['Fecha de la pueba'].str.replace('/2022', '/22')\n",
    "date_ori['Fecha de la pueba'] = date_ori['Fecha de la pueba'].str.replace('/2023', '/23')\n",
    "date_ori['Fecha de la pueba'] = date_ori['Fecha de la pueba'].str.replace('/2024', '/24')\n",
    "date_ori['Fecha de la pueba'] = date_ori['Fecha de la pueba'].str.replace('/2025', '/25')\n",
    "# Veo que campos de mes superan el 12 cuando no deberían e intercambio el día por el mes para ajustarme al formato dd-mm-yy\n",
    "date_ori[['dd', 'mm', 'yy']] = date_ori['Fecha de la pueba'].str.split('/', expand=True)\n",
    "date_ori['mmint'] = date_ori['mm'].astype(int)\n",
    "date_ori.loc[date_ori['mmint'] > 12, 'Fecha de la pueba'] = date_ori['mm'].astype(str) + '-' + date_ori['dd'].astype(str) + '-' + date_ori['yy'].astype(str)\n",
    "\n",
    "# comprueba que están en el formado adecuado\n",
    "date_ori['ori_temp_date'] = date_ori['Fecha de la pueba']\n",
    "date_ori_errors_num = date_ori[['ori_temp_date']].loc[pd.to_datetime(date_ori['ori_temp_date'], format='%d/%m/%y', errors='coerce').isnull()].shape[0]\n",
    "\n",
    "if date_ori_errors_num > 0:\n",
    "    print('>>> Fechas formato dd/mm/yy: ')\n",
    "    print(date_ori[['ori_temp_id', 'ori_temp_date', 'Fecha de la pueba']].loc[pd.to_datetime(date_ori['ori_temp_date'], format='%d/%m/%y', errors='coerce').isnull()])\n",
    "    # Me quedo con los errores para mandarlos a micro\n",
    "    date_ori_errors = date_ori[['ori_temp_id', 'ori_temp_date', 'Fecha de la pueba']].loc[pd.to_datetime(date_ori['ori_temp_date'], format='%d/%m/%y', errors='coerce').isnull()]\n",
    "    date_ori['ori_temp_date'] = pd.to_datetime(date_ori['ori_temp_date'], format='%d/%m/%y', errors='coerce')\n",
    "else:\n",
    "    print('>>> Fechas formato OK')\n",
    "    date_ori['ori_temp_date'] = pd.to_datetime(date_ori['ori_temp_date'], format='%d/%m/%y')\n",
    "\n",
    "# Comprueba que no hay fechas del futuro\n",
    "today = datetime.datetime.now()\n",
    "if date_ori.loc[date_ori['ori_temp_date'] > today].shape[0]:\n",
    "    print('>>> Fechas futuro: ', date_ori.loc[date_ori['ori_temp_date'] > today].shape[0])\n",
    "    print(date_ori[['ori_temp_id', 'NUHSA', 'Fecha de la pueba', 'ori_temp_date']].loc[date_ori['ori_temp_date'] > today])\n",
    "    date_ori_errors = pd.concat([date_ori_errors, date_ori[['ori_temp_id', 'NUHSA', 'Fecha de la pueba', 'ori_temp_date']].loc[date_ori['ori_temp_date'] > today]])\n",
    "    date_ori['ori_temp_date'] = date_ori['ori_temp_date'].loc[date_ori['ori_temp_date'] <= today]\n",
    "else:\n",
    "    print('>>> Fechas futuro OK')\n",
    "# Comprueba que no hay fechas anteriores al 1 enero de 2020\n",
    "if date_ori.loc[date_ori['ori_temp_date'] < '2020-01-01'].shape[0]:\n",
    "    print('>>> Fechas antiguas :')\n",
    "    print(date_ori[['ori_temp_id', 'NUHSA', 'Fecha de la pueba', 'ori_temp_date']].loc[date_ori['ori_temp_date'] < '2020-01-01'])\n",
    "    date_ori_errors = pd.concat([date_ori_errors, date_ori[['ori_temp_id', 'NUHSA', 'Fecha de la pueba', 'ori_temp_date']].loc[date_ori['ori_temp_date'] < '2020-01-01']])\n",
    "    date_ori['ori_temp_date'] = date_ori['ori_temp_date'].loc[date_ori['ori_temp_date'] >= '2020-01-01']\n",
    "else:\n",
    "    print('>>> Fechas antiguas OK')\n",
    "date_ori_errors.to_csv('date_ori_errors.tsv', sep='\\t', index=False)\n",
    "\n",
    "# elimino las fechas duplicadas con mismo id y nuhsa\n",
    "date_ori.drop(date_ori.loc[date_ori[['ori_temp_date', 'ori_temp_id']].duplicated(keep='first')].index, inplace=True)\n",
    "date_ori.drop(date_ori.loc[date_ori[['NUHSA', 'ori_temp_id']].duplicated(keep='first')].index, inplace=True)\n",
    "\n",
    "# elimino los números de laboratorio con diferente NUHSA\n",
    "ids = set(date_ori.loc[date_ori['ori_temp_id'].duplicated(keep=False)].sort_values('ori_temp_id')['ori_temp_id'].values)\n",
    "for i in ids:\n",
    "#     print(i)\n",
    "    num_dif_nuhsa = len(set(date_ori.loc[date_ori['ori_temp_id'] == i]['NUHSA'].values))\n",
    "    if num_dif_nuhsa > 1:\n",
    "#         print(num_dif_nuhsa)\n",
    "        date_ori.drop(date_ori.loc[date_ori['ori_temp_id'] == i].index, inplace=True)\n",
    "\n",
    "# mergeo las tablas de samples y microbiología oriental\n",
    "print(date_ori.columns)\n",
    "samples = samples.merge(date_ori[['ori_temp_date', 'ori_temp_id']], how='left', left_on='laboratoryID', right_on='ori_temp_id')\n",
    "samples['fecha_micro_husc'] = samples['ori_temp_date']\n",
    "\n",
    "# Unifico las fechas procedentes de microbiología oriental y  occidental\n",
    "samples.loc[(samples['ori_temp_date'].notnull()) & (samples['Fecha_micro'].isnull()), 'Fecha_micro'] = samples['ori_temp_date']\n",
    "\n",
    "# genero una tabla con las muestras que no tienen fecha de mircrobiologia\n",
    "samples.loc[samples['Fecha_micro'].isnull(), ['sample', 'laboratoryID', 'NUHSA', 'batch', 'Fecha Inicio Síntomas (dd/mm/aaaa)', 'Fecha_micro']].to_excel('muestras_sin_fecha_micro.xlsx')\n",
    "\n",
    "# unifico columnas y elimino las sobrantes de las dos tablas\n",
    "samples.loc[samples['Fecha_micro'].notnull(), 'fecha_nextstrain'] = samples['Fecha_micro']\n",
    "samples.drop(columns=['ori_temp_date', 'ori_temp_id'], inplace=True)\n",
    "\n",
    "samples.loc[samples.duplicated(keep=False)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23838, 75)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.loc[samples['batch'].str.contains('huvr')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################\n",
      "# Málaga (Microbiología HRUM)           #\n",
      "#########################################\n",
      ">>> Fechas futuro OK\n",
      ">>> Fechas antiguas OK\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "### Parseo las las fechas y los labID del fichero de HRUM ###\n",
    "#############################################################\n",
    "print('#########################################')\n",
    "print('# Málaga (Microbiología HRUM)           #')\n",
    "print('#########################################')\n",
    "errors = []\n",
    "\n",
    "date_malaga_path = f'{ssp}/metadata/hrum/HRUM_INFORME_latest.csv'\n",
    "date_malaga = pd.read_csv(date_malaga_path, sep=';', dtype=str)\n",
    "date_malaga = date_malaga[['Petición', 'Fecha registro']]\n",
    "\n",
    "# Guardo las muestras sin fecha en un dataframe para reportar errores\n",
    "errors.append(date_malaga.loc[date_malaga['Fecha registro'].isnull()])\n",
    "\n",
    "# transformo las fechas a datetime y me quedo con los errore\n",
    "errors.append(date_malaga.loc[pd.to_datetime(date_malaga['Fecha registro'], format='%m/%d/%Y', errors='coerce').isnull()])\n",
    "date_malaga['Fecha registro'] = pd.to_datetime(date_malaga['Fecha registro'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Comprueba que no hay fechas del futuro\n",
    "today = datetime.datetime.now()\n",
    "if date_malaga.loc[date_malaga['Fecha registro'] > today].shape[0]:\n",
    "    print('>>> Fechas futuro: ', date_malaga.loc[date_malaga['Fecha registro'] > today].shape[0])\n",
    "    print(date_malaga.loc[date_malaga['Fecha registro'] > today])\n",
    "    errors.append(date_malaga.loc[date_malaga['Fecha registro'] > today])\n",
    "    date_malaga['Fecha registro'] = date_malaga['Fecha registro'].loc[date_malaga['Fecha registro'] <= today]\n",
    "else:\n",
    "    print('>>> Fechas futuro OK')\n",
    "    \n",
    "# Comprueba que no hay fechas anteriores al 1 enero de 2020\n",
    "if date_malaga.loc[date_malaga['Fecha registro'] < '2020-01-01'].shape[0]:\n",
    "    print('>>> Fechas antiguas :')\n",
    "    print(date_malaga.loc[date_malaga['Fecha registro'] < '2020-01-01'])\n",
    "    errors.append(date_malaga.loc[date_malaga['Fecha registro'] < '2020-01-01'])\n",
    "    date_malaga['Fecha registro'] = date_malaga['Fecha registro'].loc[date_malaga['Fecha registro'] >= '2020-01-01']\n",
    "else:\n",
    "    print('>>> Fechas antiguas OK')\n",
    "\n",
    "# mergeo las tablas de samples y microbiología hurm\n",
    "samples = samples.merge(date_malaga, how='left', left_on='laboratoryID', right_on='Petición')\n",
    "samples['fecha_micro_hurm'] = samples['Fecha registro']\n",
    "\n",
    "# Unifico las fechas procedentes de microbiología oriental y  occidental\n",
    "samples.loc[(samples['Fecha registro'].notnull()) & (samples['Fecha_micro'].isnull()), 'Fecha_micro'] = samples['Fecha registro']\n",
    "\n",
    "# unifico columnas y elimino las sobrantes de las dos tablas\n",
    "samples.loc[samples['Fecha_micro'].notnull(), 'fecha_nextstrain'] = samples['Fecha_micro']\n",
    "samples.drop(columns=['Petición', 'Fecha registro'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23838, 76)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.loc[samples['batch'].str.contains('huvr')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################\n",
      "Andalucia Montañosa (Microbiología HUVN)\n",
      "#########################################\n",
      ">>> Errors in date format %d/%m/%Y: 2\n",
      ">>> Fechas futuro OK\n",
      ">>> Fechas antiguas OK\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Actualizo las fechas según los datos de Microbiología \n",
    "############ Andalucía Montañosa #######################\n",
    "print('#########################################')\n",
    "print('Andalucia Montañosa (Microbiología HUVN)')\n",
    "print('#########################################')\n",
    "date_monta_path = f'{metadata_path}/huvn/INFORME_ACUMULADO_SECUENCIACION_SARS-COV-2_actualizado_latest.csv'\n",
    "date_monta = pd.read_csv(date_monta_path, sep=';', dtype=str)\n",
    "date_monta['date_huvn'] = date_monta['Fecha de toma de muestra']\n",
    "date_monta.loc[date_monta['date_huvn'].isnull(), 'date_huvn'] = date_monta['Fecha de recepción de la muestra']\n",
    "\n",
    "# transformo las fechas a datetime y me quedo con los errore\n",
    "format_errors_num = date_monta.loc[pd.to_datetime(date_monta['date_huvn'], format='%d/%m/%Y', errors='coerce').isnull()].shape[0]\n",
    "print(f'>>> Errors in date format %d/%m/%Y: {format_errors_num}')\n",
    "date_monta['date_huvn'] = pd.to_datetime(date_monta['date_huvn'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Comprueba que no hay fechas del futuro\n",
    "today = datetime.datetime.now()\n",
    "if date_monta.loc[date_monta['date_huvn'] > today].shape[0]:\n",
    "    print('>>> Fechas futuro: ', date_monta.loc[date_monta['date_huvn'] > today].shape[0])\n",
    "    print(date_monta.loc[date_monta['date_huvn'] > today])\n",
    "    errors.append(date_monta.loc[date_monta['date_huvn'] > today])\n",
    "    date_monta['date_huvn'] = date_monta['date_huvn'].loc[date_monta['date_huvn'] <= today]\n",
    "else:\n",
    "    print('>>> Fechas futuro OK')\n",
    "    \n",
    "# Comprueba que no hay fechas anteriores al 1 enero de 2020\n",
    "if date_monta.loc[date_monta['date_huvn'] < '2020-01-01'].shape[0]:\n",
    "    print('>>> Fechas antiguas :')\n",
    "    print(date_monta.loc[date_monta['date_huvn'] < '2020-01-01'])\n",
    "    errors.append(date_monta.loc[date_monta['date_huvn'] < '2020-01-01'])\n",
    "    date_monta['date_huvn'] = date_monta['date_huvn'].loc[date_monta['date_huvn'] >= '2020-01-01']\n",
    "else:\n",
    "    print('>>> Fechas antiguas OK')\n",
    "\n",
    "# mergeo las tablas de samples y microbiología huvn\n",
    "samples = samples.merge(date_monta[['date_huvn', 'Número']], how='left', left_on='laboratoryID', right_on='Número')\n",
    "samples['fecha_micro_huvn'] = samples['date_huvn']\n",
    "\n",
    "# Unifico las fechas procedentes de microbiología oriental y  occidental\n",
    "samples.loc[(samples['date_huvn'].notnull()) & (samples['Fecha_micro'].isnull()), 'Fecha_micro'] = samples['date_huvn']\n",
    "\n",
    "# Unifico columnas y elimino las sobrantes de las dos tablas\n",
    "samples.loc[samples['Fecha_micro'].notnull(), 'fecha_nextstrain'] = samples['Fecha_micro']\n",
    "samples.drop(columns=['Número', 'date_huvn'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>NUHSA</th>\n",
       "      <th>laboratoryID</th>\n",
       "      <th>Fecha del caso</th>\n",
       "      <th>Fecha_diag</th>\n",
       "      <th>Fecha_micro_huvr</th>\n",
       "      <th>fecha_micro_husc</th>\n",
       "      <th>fecha_micro_hurm</th>\n",
       "      <th>fecha_micro_huvn</th>\n",
       "      <th>Fecha_micro</th>\n",
       "      <th>fecha_nextstrain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31536</th>\n",
       "      <td>AND31602</td>\n",
       "      <td>AN0353134056</td>\n",
       "      <td>11306019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/04/2020</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31541</th>\n",
       "      <td>AND31607</td>\n",
       "      <td>AN0368827545</td>\n",
       "      <td>11434795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/04/2020</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33569</th>\n",
       "      <td>AND33684</td>\n",
       "      <td>AN0014340944</td>\n",
       "      <td>80966346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19/05/2022</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-05-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33580</th>\n",
       "      <td>AND33695</td>\n",
       "      <td>AN0015762804</td>\n",
       "      <td>80966357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25/01/2022</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33621</th>\n",
       "      <td>AND33736</td>\n",
       "      <td>AN0533710569</td>\n",
       "      <td>28257783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/11/2020</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33632</th>\n",
       "      <td>AND33747</td>\n",
       "      <td>AN0698638558</td>\n",
       "      <td>50548159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/10/2020</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-10-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33633</th>\n",
       "      <td>AND33748</td>\n",
       "      <td>AN0817235408</td>\n",
       "      <td>50548194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16/02/2022</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-02-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sample         NUHSA laboratoryID Fecha del caso  Fecha_diag  \\\n",
       "31536  AND31602  AN0353134056     11306019            NaN  05/04/2020   \n",
       "31541  AND31607  AN0368827545     11434795            NaN  05/04/2020   \n",
       "33569  AND33684  AN0014340944     80966346            NaN  19/05/2022   \n",
       "33580  AND33695  AN0015762804     80966357            NaN  25/01/2022   \n",
       "33621  AND33736  AN0533710569     28257783            NaN  05/11/2020   \n",
       "33632  AND33747  AN0698638558     50548159            NaN  17/10/2020   \n",
       "33633  AND33748  AN0817235408     50548194            NaN  16/02/2022   \n",
       "\n",
       "      Fecha_micro_huvr fecha_micro_husc fecha_micro_hurm fecha_micro_huvn  \\\n",
       "31536              NaT              NaT              NaT              NaT   \n",
       "31541              NaT              NaT              NaT              NaT   \n",
       "33569              NaT              NaT              NaT              NaT   \n",
       "33580              NaT              NaT              NaT              NaT   \n",
       "33621              NaT              NaT              NaT              NaT   \n",
       "33632              NaT              NaT              NaT              NaT   \n",
       "33633              NaT              NaT              NaT              NaT   \n",
       "\n",
       "      Fecha_micro fecha_nextstrain  \n",
       "31536         NaT       2020-04-05  \n",
       "31541         NaT       2020-04-05  \n",
       "33569         NaT       2022-05-19  \n",
       "33580         NaT       2022-01-25  \n",
       "33621         NaT       2020-11-05  \n",
       "33632         NaT       2020-10-17  \n",
       "33633         NaT       2022-02-16  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################\n",
    "####### test de fechas ########\n",
    "date_columns=[\n",
    "    'sample','NUHSA', 'laboratoryID',\n",
    "    'Fecha del caso', # Salud Publica\n",
    "    'Fecha_diag', # Salud Publica,\n",
    "    'Fecha_micro_huvr',\n",
    "    'fecha_micro_husc',\n",
    "    'fecha_micro_hurm',\n",
    "    'fecha_micro_huvn',\n",
    "    'Fecha_micro', # Fechas de micro unificadas\n",
    "    'fecha_nextstrain'\n",
    "]\n",
    "\n",
    "test_ids = ['AND31602', 'AND33747', 'AND33736', 'AND33695', 'AND33748', 'AND33684', 'AND31607'] # esta muestra es del virgen de las nieves con fecha de salud publica del 20 y de micro del 22\n",
    "\n",
    "samples.loc[samples['sample'].isin(test_ids) , date_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lou_dates = samples[['sample', 'fecha_nextstrain']].rename(columns={'fecha_nextstrain':'date'})\n",
    "lou_dates.to_csv('./rest_waves_sample_date.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43393, 77)\n",
      "(43362, 3)\n",
      "(44156, 17)\n",
      "(43885, 16)\n"
     ]
    }
   ],
   "source": [
    "print(samples.shape)\n",
    "print(mapping.shape)\n",
    "print(metadata.shape)\n",
    "print(lineage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43393, 77)\n",
      "(43362, 3)\n",
      "(44156, 17)\n",
      "(43885, 16)\n"
     ]
    }
   ],
   "source": [
    "print(samples.shape)\n",
    "print(mapping.shape)\n",
    "print(metadata.shape)\n",
    "print(lineage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AND13515',\n",
       " 'AND13516',\n",
       " 'AND13517',\n",
       " 'AND13518',\n",
       " 'AND13519',\n",
       " 'AND13520',\n",
       " 'AND13521',\n",
       " 'AND13522',\n",
       " 'AND13523',\n",
       " 'AND13524',\n",
       " 'AND13525',\n",
       " 'AND13526',\n",
       " 'AND13527',\n",
       " 'AND13528',\n",
       " 'AND13529',\n",
       " 'AND13530',\n",
       " 'AND13531',\n",
       " 'AND13532',\n",
       " 'AND13533',\n",
       " 'AND13534',\n",
       " 'AND13535',\n",
       " 'AND13536',\n",
       " 'AND13537'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(samples['sample'].values) - set(mapping['sample'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50290/1549082106.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dup = dup.fillna(0)\n",
      "/tmp/ipykernel_50290/1549082106.py:3: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dup = dup.fillna(0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>laboratoryID</th>\n",
       "      <th>NUHSA</th>\n",
       "      <th>selected_for_nextstrain</th>\n",
       "      <th>batch</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>Edad (años)</th>\n",
       "      <th>fecha_nextstrain</th>\n",
       "      <th>Fecha del caso</th>\n",
       "      <th>hospital</th>\n",
       "      <th>hospital de referencia</th>\n",
       "      <th>Municipio</th>\n",
       "      <th>provincia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratoryID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1512915</th>\n",
       "      <td>{AND26568}</td>\n",
       "      <td>2</td>\n",
       "      <td>{AN0232313886}</td>\n",
       "      <td>{no}</td>\n",
       "      <td>{AND_232_huvr_290622}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{2023-08-24 00:00:00, 2022-06-16 00:00:00}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{Hospital Universitario Juan Ramón Jiménez}</td>\n",
       "      <td>{HUVR}</td>\n",
       "      <td>{Aljaraque}</td>\n",
       "      <td>{Huelva}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sample  laboratoryID           NUHSA  \\\n",
       "laboratoryID                                             \n",
       "1512915       {AND26568}             2  {AN0232313886}   \n",
       "\n",
       "             selected_for_nextstrain                  batch Sexo Edad (años)  \\\n",
       "laboratoryID                                                                   \n",
       "1512915                         {no}  {AND_232_huvr_290622}  {0}         {0}   \n",
       "\n",
       "                                        fecha_nextstrain Fecha del caso  \\\n",
       "laboratoryID                                                              \n",
       "1512915       {2023-08-24 00:00:00, 2022-06-16 00:00:00}            {0}   \n",
       "\n",
       "                                                 hospital  \\\n",
       "laboratoryID                                                \n",
       "1512915       {Hospital Universitario Juan Ramón Jiménez}   \n",
       "\n",
       "             hospital de referencia    Municipio provincia  \n",
       "laboratoryID                                                \n",
       "1512915                      {HUVR}  {Aljaraque}  {Huelva}  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra los duplicados\n",
    "dup = samples.loc[samples['sample'].duplicated(keep=False)]\n",
    "dup = dup.fillna(0)\n",
    "dup.drop_duplicates(keep=False, inplace=True)\n",
    "# dup = dup.loc[dup['laboratoryID'].duplicated(keep=False)]\n",
    "\n",
    "dup[['sample', \n",
    "     'laboratoryID', \n",
    "     'NUHSA',\n",
    "     'selected_for_nextstrain',\n",
    "     'batch',\n",
    "     'Sexo',\n",
    "     'Edad (años)',\n",
    "     'fecha_nextstrain',\n",
    "     'Fecha del caso',\n",
    "     'hospital',\n",
    "     'hospital de referencia',\n",
    "     'Municipio',\n",
    "     'provincia'\n",
    "    ]].groupby('laboratoryID').agg({\n",
    "                            'sample':set,\n",
    "                            'laboratoryID':'count',\n",
    "                            'NUHSA':set,\n",
    "                            'selected_for_nextstrain':set,\n",
    "                            'batch':set,\n",
    "                            'Sexo':set,\n",
    "                            'Edad (años)':set,\n",
    "                            'fecha_nextstrain':set,\n",
    "                            'Fecha del caso':set,\n",
    "                            'hospital':set,\n",
    "                            'hospital de referencia':set,\n",
    "                            'Municipio':set,\n",
    "                            'provincia':set\n",
    "                           }).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_csv(f'{ssp}/analysis_MN908947/other/complete_samples_metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auspice: 43393\n",
      "auspice selected for nextstrain: 35803\n",
      "auspice added first wave: 36820\n",
      "auspice with date: 35953\n",
      "auspice after date-lineage filter: 35892\n"
     ]
    }
   ],
   "source": [
    "auspice = samples[\n",
    "    [\n",
    "    'sample',\n",
    "    'laboratoryID',\n",
    "    'NUHSA',    \n",
    "    'fecha_nextstrain',\n",
    "    'País del caso',\n",
    "    'provincia',\n",
    "    'Municipio',\n",
    "    'origen',\n",
    "    'hospital',\n",
    "    'hospital de referencia',\n",
    "    'Centro de salud',\n",
    "    'centro_secuenciacion',\n",
    "    'lineage',\n",
    "    'clade',\n",
    "    'clade_display',\n",
    "    'nextstrain_genome',\n",
    "    'selected_for_nextstrain',\n",
    "    'batch',\n",
    "    'percentage_reference_genome_in_consensus_ivar',\n",
    "    'seqPlatform'\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "auspice.rename(columns={\n",
    "    'sample':'strain',\n",
    "    'fecha_nextstrain':'date',\n",
    "    'País del caso':'country',\n",
    "    'Municipio':'location',\n",
    "    'origen':'source',\n",
    "    'centro_secuenciacion':'laboratorio_secuenciación'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "### creo las columnas necesarias para nextstrain\n",
    "auspice['region'] = 'Europe'\n",
    "auspice['country'] = 'Spain'\n",
    "auspice['location'] = auspice['location'].str.strip()\n",
    "\n",
    "# conversion de nombres de localidades\n",
    "location_names = f'./data/location_names_dict.tsv'\n",
    "loc = {}\n",
    "with open(location_names,'r') as f:\n",
    "    for line in f: \n",
    "        line = line.rstrip()\n",
    "        words = line.split('\\t')\n",
    "        loc[words[0]] = words[1]\n",
    "for l in loc.keys():\n",
    "    auspice.loc[:,'location'] = auspice['location'].replace(l, loc[l])\n",
    "\n",
    "print(f'auspice: {auspice.shape[0]}')\n",
    "\n",
    "\n",
    "\n",
    "### me quedo solo con las seleccionadas para nextstrains\n",
    "auspice = auspice.loc[auspice['selected_for_nextstrain'] == 'yes']\n",
    "print(f'auspice selected for nextstrain: {auspice.shape[0]}')\n",
    "\n",
    "\n",
    "### concateno con la primera tanda de relcov\n",
    "primera = pd.read_csv(f'{base}/other_data/1wave_data/metadata.tsv', sep='\\t')\n",
    "primera['1wave'] = 'yes'\n",
    "primera['nextstrain_genome'] = np.nan\n",
    "primera['date'] = pd.to_datetime(primera['date'], format='%Y-%m-%d')\n",
    "### elimino los clados y linages ya que voy a mergearlos con unos más recientes\n",
    "primera.drop(columns=['clade', 'lineage'], inplace=True)\n",
    "### mergeo la primera ola con los clados de nextclade actualizados\n",
    "primera = primera.merge(nextclade[['seqName', 'clade', 'clade_display']], how='left', left_on='strain', right_on='seqName')\n",
    "### mergeo la primera ola con los linages actualizados\n",
    "primera = primera.merge(lineage[['taxon', 'lineage']], how='left', left_on='strain', right_on='taxon')\n",
    "### eliminio los indices creados con los mergeos de nextclade y lineage\n",
    "primera.drop(columns=['seqName', 'taxon'], inplace=True)\n",
    "### concateno con las muestras seleccionadas para auspice\n",
    "auspice = pd.concat([auspice, primera])\n",
    "print(f'auspice added first wave: {auspice.shape[0]}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### me quedo con la que tienen fecha\n",
    "auspice = auspice.loc[auspice['date'].notnull()]\n",
    "print(f'auspice with date: {auspice.shape[0]}')\n",
    "\n",
    "\n",
    "\n",
    "### filtros de fecha por linaje\n",
    "err_date_lineage_alpha = auspice.loc[(auspice['date'] < '2020-12-01') & (auspice['lineage'] == 'B.1.1.7')]\n",
    "err_date_lineage_delta = auspice.loc[(auspice['date'] < '2021-05-01') & (auspice['lineage'].str.startswith('AY'))]\n",
    "err_date_lineage_delta_2 = auspice.loc[(auspice['date'] < '2021-05-01') & (auspice['lineage'] == 'B.1.617.2')]\n",
    "err_date_lineage_omicron = auspice.loc[(auspice['date'] < '2021-12-01') & (auspice['lineage'].str.startswith('BA'))]\n",
    "err_date_lineage = pd.concat([err_date_lineage_alpha, err_date_lineage_delta, err_date_lineage_delta_2, err_date_lineage_omicron])\n",
    "err_date_lineage.to_excel('error_date_lineage.xlsx', index=False)\n",
    "\n",
    "auspice = auspice.loc[~auspice['strain'].isin(err_date_lineage['strain'])]\n",
    "print(f'auspice after date-lineage filter: {auspice.shape[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auspice after AND id filter: 35606\n",
      "auspice without strain duplicated: 35604\n",
      "auspice without future dates: 35604\n",
      "auspice with location: 32341\n",
      "auspice without SAS-imputed: 32194\n"
     ]
    }
   ],
   "source": [
    "### filtro de muestras extrañas a partir de AND id (son recombinantes fuera de fecha)\n",
    "# ANDid_todel = [\n",
    "#     'AND31602',\n",
    "#     'AND31607',\n",
    "#     'AND33747',\n",
    "#     'AND33736',\n",
    "#     'AND33706',\n",
    "#     'AND31606',\n",
    "#     'AND33695',\n",
    "#     'AND33748',\n",
    "#     'AND33684'\n",
    "# ]\n",
    "# auspice = auspice.loc[~auspice['strain'].isin(ANDid_todel)].copy()\n",
    "\n",
    "##### elimino de la selección las muestras de Jerez ####\n",
    "# and_jerez = ['AND32737', 'AND32242'] # No tienen fechas adecuadas parece que se han secuenciado meses despues del diagnostico\n",
    "# auspice = auspice.loc[~auspice['strain'].isin(and_jerez)]\n",
    "# print(f'auspice after AND id filter: {auspice.shape[0]}')\n",
    "\n",
    "### Elimino las muestras del fichero remove_SARSCOV2_samples_outliers.txt\n",
    "### Están incluidos todos los filtros que se basan en AND_id\n",
    "andid_todel = []\n",
    "with open(nextstrain_data / 'remove_SARSCOV2_samples_outliers.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        andid_todel.append(line)\n",
    "auspice = auspice.loc[~auspice['strain'].isin(andid_todel)]\n",
    "print(f'auspice after AND id filter: {auspice.shape[0]}')\n",
    "\n",
    "### elimino duplicados\n",
    "auspice.drop_duplicates('strain', keep='first', inplace=True)\n",
    "print(f'auspice without strain duplicated: {auspice.shape[0]}')\n",
    "\n",
    "### Compruebo que no existen fechas del futuro\n",
    "today = datetime.datetime.now()\n",
    "auspice = auspice.loc[auspice['date'] < today]\n",
    "print(f'auspice without future dates: {auspice.shape[0]}')\n",
    "\n",
    "### string vacía para las muestras que no tienen batch\n",
    "auspice.loc[auspice['batch'].isnull(), 'batch'] = ''\n",
    "\n",
    "\n",
    "### hago una copia para guardar lo que necesita Javi para sus gráficas\n",
    "### samples selected for nextstrain with date\n",
    "today = datetime.datetime.now().strftime('%d%m%y')\n",
    "web_path = f'{ssp}/analysis_MN908947/reports/web/'\n",
    "agg_noloc = auspice[[\n",
    "                    'strain',\n",
    "                    'batch',\n",
    "                    'date',\n",
    "                    'lineage',\n",
    "                    'location',\n",
    "                    'provincia',\n",
    "                    'hospital',\n",
    "                    'hospital de referencia'\n",
    "                ]].copy()\n",
    "agg_noloc.to_csv(f'{web_path}agregado_web_noloc_{today}.tsv', sep='\\t', index=False)\n",
    "agg_noloc.loc[agg_noloc['batch'].str.contains('husc')].to_csv(f'{web_path}agregado_web_noloc_husc_{today}.tsv', sep='\\t', index=False)\n",
    "agg_noloc.loc[agg_noloc['batch'].str.contains('huvn')].to_csv(f'{web_path}agregado_web_noloc_huvn_{today}.tsv', sep='\\t', index=False)\n",
    "agg_noloc.loc[agg_noloc['batch'].str.contains('huvr')].to_csv(f'{web_path}agregado_web_noloc_huvr_{today}.tsv', sep='\\t', index=False)\n",
    "\n",
    "### me quedo con las que tienen localizacion\n",
    "auspice = auspice.loc[auspice['location'].notnull()]\n",
    "print(f'auspice with location: {auspice.shape[0]}')\n",
    "\n",
    "### elimino las que son SAS-imputed\n",
    "auspice = auspice.loc[auspice['source'] != 'SAS-imputed']\n",
    "print(f'auspice without SAS-imputed: {auspice.shape[0]}')\n",
    "\n",
    "\n",
    "### hago una copia del agregado web antiguo por retro compatibilidad\n",
    "agg = auspice[[\n",
    "                'strain',\n",
    "                'batch',\n",
    "                'date',\n",
    "                'lineage',\n",
    "                'location',\n",
    "                'provincia',\n",
    "                'hospital',\n",
    "                'hospital de referencia'\n",
    "            ]].copy()\n",
    "\n",
    "agg.to_csv(f'{web_path}agregado_web_{today}.tsv', sep='\\t', index=False)\n",
    "agg.loc[agg['batch'].str.contains('husc')].to_csv(f'{web_path}agregado_web_husc_{today}.tsv', sep='\\t', index=False)\n",
    "agg.loc[agg['batch'].str.contains('huvn')].to_csv(f'{web_path}agregado_web_huvn_{today}.tsv', sep='\\t', index=False)\n",
    "agg.loc[agg['batch'].str.contains('huvr')].to_csv(f'{web_path}agregado_web_huvr_{today}.tsv', sep='\\t', index=False)\n",
    "\n",
    "### transformo las fechas a strings\n",
    "auspice['date'] = auspice['date'].dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SAS', 'GISAID'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auspice['source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V�lor'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprueba que todas las localizaciones tienen latitud y longitud\n",
    "lat_long = pd.read_csv('config/lat_long.tsv', sep='\\t', header=None)\n",
    "lat_long = set(lat_long[1].values)\n",
    "locs = set(auspice['location'].values)\n",
    "locs - lat_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### marco las muestras de los últmos 4 meses \n",
    "###### para no filtrarlas con augur filter\n",
    "months = datetime.date.today() - datetime.timedelta(4*365/12)\n",
    "months = months.strftime(format=\"%Y-%m-%d\")\n",
    "auspice.loc[pd.to_datetime(auspice['date']) >= months, 'include'] = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabla para los datos de las gráficas de la web para javier sin filtros\n",
    "agregado_nofilter = samples[[\n",
    "    'sample', \n",
    "    'lineage',\n",
    "    'laboratoryID', \n",
    "    'batch', \n",
    "    'hospital',\n",
    "    'hospital de referencia',\n",
    "    'fecha_nextstrain'\n",
    "        ]].copy()\n",
    "agregado_nofilter_husc = agregado_nofilter.loc[agregado_nofilter['batch'].str.contains('husc')].copy()\n",
    "agregado_nofilter_huvn = agregado_nofilter.loc[agregado_nofilter['batch'].str.contains('huvn')].copy()\n",
    "agregado_nofilter_huvr = agregado_nofilter.loc[agregado_nofilter['batch'].str.contains('huvr')].copy()\n",
    "agregado_nofilter.to_csv(f'{ssp}/analysis_MN908947/reports/web/agregado_without_QC_{today}.tsv', \n",
    "                         sep='\\t', \n",
    "                         index=False)\n",
    "agregado_nofilter_husc.to_csv(f'{ssp}/analysis_MN908947/reports/web/agregado_without_QC_husc_{today}.tsv', \n",
    "                         sep='\\t', \n",
    "                         index=False)\n",
    "agregado_nofilter_huvn.to_csv(f'{ssp}/analysis_MN908947/reports/web/agregado_without_QC_huvn_{today}.tsv', \n",
    "                         sep='\\t', \n",
    "                         index=False)\n",
    "agregado_nofilter_huvr.to_csv(f'{ssp}/analysis_MN908947/reports/web/agregado_without_QC_huvr_{today}.tsv', \n",
    "                         sep='\\t', \n",
    "                         index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32194 32194\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# genero el multifasta con las secuencias consenso\n",
    "consensus_seq = {}\n",
    "key = \"\"\n",
    "\n",
    "# obtenemos los path de las secuencias consenso de las muestras que tienen metadatos\n",
    "for i, path in auspice[['strain', 'nextstrain_genome']].loc[auspice['nextstrain_genome'].notnull()].values:\n",
    "#     print(i, path)\n",
    "    if path == 'unknown':\n",
    "        continue\n",
    "    path = path.replace('/analysis_MN90894/', '/analysis_MN908947/')\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.read().splitlines()\n",
    "        for line in lines:\n",
    "            if line.startswith(r'>'):\n",
    "                key = i\n",
    "                consensus_seq[key] = \"\"\n",
    "                continue\n",
    "            consensus_seq[key] += line\n",
    "            \n",
    "# obtengo las secuencias de las muestras de la primera ola (propias más GISAID)\n",
    "wave1_seq_fasta = f'{base}/other_data/1wave_data/andalusia_seq.fasta'\n",
    "ids_1wave = auspice.loc[auspice['1wave'] == 'yes', 'strain'].values\n",
    "wave1_seq = fasta_get_by_name(fasta=wave1_seq_fasta, ids=ids_1wave)\n",
    "\n",
    "\n",
    "# obtengo las secuencias de las muestras de GISAID ahora mismo no se usa\n",
    "# gisaid = f'./data/raw_gisaid/2020-07-06/'            \n",
    "# ids_gi = auspice.loc[auspice['source'] == 'GISAID', 'strain'].values\n",
    "# gi_seq = fasta_get_by_name(fasta=f'{gisaid}sequences.fasta', ids=ids_gi)\n",
    "##############\n",
    "\n",
    "seq = dict(consensus_seq, **wave1_seq)\n",
    "with open(f'./data/sequence.fasta', 'w') as file:\n",
    "    for k in seq:\n",
    "        file.write(f'>{k}\\n')\n",
    "        file.write(f'{seq[k]}\\n')\n",
    "        \n",
    "# Comprueba que todos los id de las secuencias están dentro de los metadatos\n",
    "print(len(seq.keys()), len(auspice['strain']))\n",
    "print(set(seq.keys()) - set(auspice['strain']))\n",
    "print(set(auspice['strain']) - set(seq.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auspice.to_csv('./data/auspice_metadata.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo las secuencias consenso de todas las muestran hayan o no pasado el control de calidad\n",
    "consensus_seq = {}\n",
    "for i, path in samples[['sample', 'nextstrain_genome']].loc[samples['nextstrain_genome'].notnull()].values:\n",
    "#     print(i, path)\n",
    "    if path == 'unknown' or path == '-':\n",
    "        continue\n",
    "    path = path.replace('/analysis_MN90894/', '/analysis_MN908947/')\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.read().splitlines()\n",
    "        for line in lines:\n",
    "            if line.startswith(r'>'):\n",
    "                key = i\n",
    "                consensus_seq[key] = \"\"\n",
    "                continue\n",
    "            consensus_seq[key] += line\n",
    "\n",
    "# obtengo las secuencias de las muestras de la primera ola (propias más GISAID)\n",
    "wave1_consensus_seqs_path = f'{base}/other_data/1wave_data/1wave_all_consensus_sequences.fasta'\n",
    "wave1_consensus_seqs = {}\n",
    "with open(wave1_consensus_seqs_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith(r'>'):\n",
    "            key = line[1:]\n",
    "            wave1_consensus_seqs[key] = ''\n",
    "            continue\n",
    "        wave1_consensus_seqs[key] += line\n",
    "            \n",
    "seq = dict(consensus_seq, **wave1_consensus_seqs)\n",
    "with open(f'./data/AND1-100_1wave_consensus_sequences.fasta', 'w') as file:\n",
    "    for k in seq:\n",
    "        file.write(f'>{k}\\n')\n",
    "        file.write(f'{seq[k]}\\n')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43529"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43529"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
